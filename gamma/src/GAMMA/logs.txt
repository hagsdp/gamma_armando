1. Using architecture PEs=168, L1=512, L2=108000, NocBW=81920000
    1. Model mnasnet with 52 layers: 1.182e+02
    2. Model ncf_m with 12 layers: 1.122e+02
    3. Model mobilenet_v2 with 52 layers: 1.154e+02
    4. Model googlenet with 59 layers: 1.140e+02
    5. Model dlrmRMC1_m with 6 layers: 9.310e+01
    6. Model alexnet with 5 layers: 1.638e+02
    7. Model transformer with 96 layers: 1.211e+02
    8. Model resnet50 with 53 layers: 1.221e+02
    9. Model wide_resnet50 with 53 layers: 1.240e+02
    10. Model manual with 3 layers: 1.377e+02
    11. Model shufflenet_v2 with 56 layers: 1.056e+02
    12. Model squeezenet with 26 layers: 1.051e+02
    13. Model try with 3 layers: 1.038e+02
    14. Model resnet18 with 20 layers: 1.093e+02
    15. Model vgg16 with 13 layers: 1.217e+02
    16. Model densenet with 160 layers: 1.150e+02
    17. Model T5_m with 6 layers: 1.156e+02
    18. Model resnext50_32x4d with 53 layers: 1.249e+02
    19. Model BERT_m with 6 layers: 1.138e+02
    20. Model ALBERT_m with 5 layers: 1.123e+02

Final result with all models and fixed hardware = 116.87665101488497

2. Using architecture PEs=200, L1=256, L2=1024, NocBW=81920000
    1. Model mnasnet with 52 layers: 5.138e+01
    2. Model ncf_m with 12 layers: 6.361e+01
    3. Model mobilenet_v2 with 52 layers: 6.143e+01
    4. Model googlenet with 59 layers: 6.726e+01
    5. Model dlrmRMC1_m with 6 layers: 4.702e+01
    6. Model alexnet with 5 layers: 5.767e+01
    7. Model transformer with 96 layers: 8.380e+01
    8. Model resnet50 with 53 layers: 6.538e+01
    9. Model wide_resnet50 with 53 layers: 6.844e+01
    10. Model manual with 3 layers: 6.739e+01
    11. Model shufflenet_v2 with 56 layers: 5.633e+01
    12. Model squeezenet with 26 layers: 5.220e+01
    13. Model try with 3 layers: 2.711e+01
    14. Model resnet18 with 20 layers: 6.474e+01
    15. Model vgg16 with 13 layers: 5.445e+01
    16. Model densenet with 160 layers: 7.396e+01
    17. Model T5_m with 6 layers: 8.792e+01
    18. Model resnext50_32x4d with 53 layers: 7.396e+01
    19. Model BERT_m with 6 layers: 7.655e+01
    20. Model ALBERT_m with 5 layers: 9.360e+01

Final result with all models and fixed hardware = 68.08087710419485

